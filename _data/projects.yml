- title: "Training and Benchmarking Neural Machine Translation Models"
  members: "Ethan Mathieu, Shankara Abbineni"
  abstract: "In this project, we ask two questions: what are the gains to fine-tuning general langauge models on translation; and can general language models, when fine-tuned, perform better on translation tasks than a model trained solely for translation. As such, we train the DeLighT transformer model for English-to-French translation and compare its BLEU performance to other neural machine translation models which we fine-tune. We find that fine-tuned general language models can perform better than language-specific models. Additionally, we build a NextJS web application to allow end users to experiment with the different models and view their performance."
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/abbinenishankara_116308_9500712_CPSC477_Final_Report.pdf?csf=1&web=1&e=ahF1g0"

- title: "Improved Protein Function Prediction by Combining Persistent Cohomology and ProteinBERT Embeddings"
  members: "Anna Su, Jason Apostol"
  abstract: "Understanding the molecular function of proteins is extremely important in elucidating their biological mechanisms and in engineering new theraputics. We present a protein function classifier combining features from both sequence and structure, through embeddings generated by a pretrained ProteinBERT model trained on ~100 M proteins supplemented with structural generated on a molecular functionspecific implementation of PersLay trained on our smaller target dataset of ~6,000 human protein structures. We show that supplementing the sequence embeddings with structural embeddings improves classifier accuracy by approximately 4 % by using a relatively small number of parameters, and demonstrate that the H_1 homology group is the most important for performance. This work has applications to drug discovery, elucidation of biological pathways, and protein engineering, as it provides a high-fidelity estimate of the role of a protein in a biological system."
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/apostoljason_148729_9500993_NLP_Final_Project%20(2).pdf?csf=1&web=1&e=cVnHaD"

- title: "Biomedical Lay Summarization"
  members: "Xincheng Cai, Mengmeng Du"
  abstract: "Biomedical research articles contain vital information for a wide audience, yet their complex language and specialized terminology often hinder comprehension for non-experts. Inspired by the BIONLP 2024 workshop, we propose a NLP solution to generate lay summaries, which are more readable to diverse audiences. We implemented two transformer-based models, specifically BART and BART-PubMed. Our study investigates the performance of these models across different biomedical topics and explores methods to improve summarization quality through definition retrieval from Webster Medical Dictionary. By enhancing the readability of biomedical publications, our work aims to promote knowledge accessibility to scientific information."
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/caixincheng_192392_9496589_CPSC577_Final_Project.pdf?csf=1&web=1&e=6qHdOP"

- title: "Advancing AI Safety in LLMs through Dynamic Multi-Agent Debates"
  members: "Vincent Li, Anna Zhang, Lindsay Chen"
  abstract: "The safety and security of large language models (LLMs) has garnered significant attention with the advent of multi-agent frameworks. Our research expands on methodologies proposed in "Combating Adversarial Attacks with Multi-Agent Debate"(1) by introducing dynamic role allocation and diversifying agent capabilities within multi-agent frameworks. These enhancements address key limitations, including static role allocation and agent homogeneity, which limit the adaptability of debates in uncovering adversarial strategies. Our proposed framework incorporates dynamic roles such as proposer, opposer, questioner, and mediator, alongside enhanced agent capabilities that allow for nuanced exploration of adversarial dialogues. The framework is implemented and trained using state-of-the-art LLMs and evaluated on existing datasets, demonstrating its effectiveness in identifying and mitigating adversarial threats in LLMs. This innovative approach advances AI safety by fostering more robust and versatile multi-agent interactions, contributing to secure and reliable LLM applications."
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/chenlindsay_108121_9496997_CPSC477_Final_Report.pdf?csf=1&web=1&e=Jt3YIA"

- title: "Transfer Learning is All You Need for Sentiment Analysis"
  members: "Minyi Chen, Zishun Zhou, Bowen Duanmu"
  abstract: "Transfer learning is a crucial technique that helps us learn from external sources, thus improving model performance on small datasets. In this paper, we work on Twitter Sentiment Datasets with three categories: Neutral, Positive, and Negative, using models like Bert and Gemma, and explore the impact of transfer learning on classification performance. We experimented with various data preprocessing strategies, such as removing stop words and special characters like emojis. We pre-trained our model on different datasets with similar or different tasks. During fine-tuning, we tried various freeze strategies as well. Our best results get 93.5% accuracy, 93.1 % recall, and 93.4 % F1 score in test set. Experimental results indicate that the performance of transfer learning is influenced by various factors, including the model, dataset relationships, and freeze strategies."
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/chenminyi_194162_9501120_NeurIPS_2023__Copy_%20(1).pdf?csf=1&web=1&e=KSrOgh" 

- title: "Deciphering Clinical Trial Reports: A Novel NLP Task and Corpus for Evidence Inference"
  members: "Xinyi Di, Chengxi Wang, Yun Yang"
  abstract: "In healthcare, accurate assessment of treatment efficacy is crucial but hindered by the complex and voluminous nature of clinical trial reports. Traditional methods fall short, highlighting the need for advanced automated solutions. Our research addresses this challenge by developing NLP models that utilize sophisticated attention mechanisms to improve the extraction and synthesis of evidence from these reports. By incorporating LoRA, we enhance the fine-tuning efficiency of our models, making large language models more accessible and effective. We evaluate our approach by comparing the performance of a BERT-based baseline model with advanced models constructed using BioBERT and ClinicalBert. This study not only advances the field of NLP in healthcare but also has the potential to revolutionize the way clinical evidence is processed, hence enhancing patient care."
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/dixinyi_168124_9488283_NLP_Final_Report.pdf?csf=1&web=1&e=WEPIz7"

- title: "Llama3-8-Bing A sarcastic language model learns from Chandler Bing"
  members: "Yuntian Liu, Zihan Dong"
  abstract: "In this project, we explored various large language models and fine tuning or alignment techniques to classify and generate sarcasm dialogues. We adopted generative AI to boost the sarcasm study and trained a sarcastic chatbot based on llama3-8B model that learned from Chandler Bing."
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/dongzihan_106584_9500895_CPSC577_SP24_Final_project.pdf?csf=1&web=1&e=cZFG6n"

- title: "Biomedical Document Summarization Models (BDSM)"
  members: Lleyton Emery, Diego Aspinwall
  abstract: In this writeup, we address the natural language processing task of BioLaySumm, which aims to generate layperson-friendly summaries of biomedical research articles. We implemented and compared various summarization approaches, including extractive and abstractive summarization models, large language models, and ensemble models. Through systematic evaluation of the relevance, readability, and factuality of summaries, we sought to identify the most effective summarization strategies for this domain, ultimately contributing to the advancement of health literacy and informed decision-making in the biomedical field.
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/emerylleyton_127205_9500990_Emery_Aspinwall_CPSC477_Final_Project_Report.pdf?csf=1&web=1&e=LYIdOF"

- title: "Models Understand Models: Predicting Unknown from What We Know"
  members: Kaiyuan Guan
  abstract: "Our research builds upon these existing frameworks with the aim of linking the assessment of abstract capabilities to the model's performance on problem sets with established ground truths. We propose a computing economical, easy to use and interpretable method to diagnose the inherent ability of any given LLMs, by leveraging the advantages of linear probes and the discoveries in self-consistency. If we want to build a model that is better than humans, it is crucial to know what leads to failure. Similar to a variety of research, we start with a crucial discovery: language models can produce well-calibrated predictions for token probabilities on-distribution (Guo et al. (2017)). Based on this, we train an MLP model based on the activations of the last token in CoT answers by the LLM, which is elicited by our compound strategy that digging the potential of few-shot, reasoning and model's intuition." 
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/guankaiyuan_192577_9500874_CPSC_477_Final_Report.pdf?csf=1&web=1&e=vxHdB3"

- title: "Predicting Primary Sub-Categories of Statistics arXiv Papers"
  members: "Ali Aldous, Eugene Han, Elder Veliz"
  abstract: We investigate the application of natural language processing techniques for automatic classification and category moderation within the arXiv repository, specifically for classifying statistics papers by primary sub-category using their titles and abstracts. Previous work has demonstrated the efficacy of fine-tuning BERT-based models for classifying arXiv papers but only on balanced datasets with broad categories such as biology and physics or distinct sub-categories under subjects other than statistics. Using a dataset of 60,648 arXiv papers within the statistics category, we experiment with TF-IDF embeddings combined with a Linear Support Vector Classifier, SPECTER2 embeddings combined with Logistic Regression, and RoBERTa models, extending past research to include imbalanced sub-categories with significant content overlap. Our results show that while fine-tuning RoBERTa substantially increases performance on unseen paper titles and abstracts, it underperforms compared to other baselines which may highlight potential shortcomings with this approach. Comprehensive details on the source code are available in the GitHub repository ehan03/arxiv-stat-nlp. Instructions for setup are provided to facilitate replication and verification by other researchers.
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/haneugene_146694_9488292_report.pdf?csf=1&web=1&e=GuAEvQ" 

- title: Synthetic Data for Cross-Domain Uncertainty Analysis
  members: Stephanie Hu
  abstract: "In many real-world applications of machine learning, obtaining labeled data in sufficient quantities can be a challenging and resource-intensive task. This project adapts a common approach in image generation and processing to address this issue. I design and train a Conditional Generative Adversarial Network (CGAN) for synthetic labeled data generation in a domain distinct from that of the training data. Unfortunately, my results show that the CGAN model architecture and finetuning methods I chose to use are not capable of generating high-quality synthetic data. They are not recognizably English and perform no better than random bag-of-words sampling. Furthermore, it appears that the conditional label has limited weight in the generator model, suggesting my model was unable to extract aspect-level features. I end by positing the limitations of my approach and suggesting further experimentation for model improvement."
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/hustephanie_109044_9491677_combinepdf.pdf?csf=1&web=1&e=Edbyf9"

- title: Reimplementation of Topic Modeling with Wasserstein Autoencoders
  members: Aryaan Khan, Yuhang Cui, Raymond Lee
  abstract: This project re-implements topic modeling with Wasserstein auto-encoders (WAE) from 2 , which have much faster training time compared to traditional topic matching using LDA, and allows direct Dirichlet distribution matching without Gaussian approximation compared to variational auto-encoders (VAE). Re-implementing WAE in the more popular PyTorch framework allows easier integration and better usability, and we will also be verifying the original papers claims by comparing the performance of WAE against LDA. Our implementation of WAE confirmed the original papers results that WAE can have performance on par or better than LDA, and have much faster training time. This project verifies the results of the original WAE paper and provides a PyTorch implementation for future use, which is available on GitHub { ^1.
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/khanaryaan_128257_9490100_Topic_Modeling_with_Wasserstein_Autoencoders-combined_1.pdf?csf=1&web=1&e=J1W6BO"

- title: "Discrimination Risks in LLMs"
  members: "Conrad Lee, Irine Juliet Otieno"
  abstract: "We explore the risks of discrimination and bias in LLMs with a short survey paper and an experiment. We find that biases in LLMs have their roots in many sources, not just in training corpora. We categorize different manifestations and targets of LLM biases as well as types of debiasing solutions. We support these findings through direct experimentation following the procedure of Dhamala et al (2021). We validate that racial biases exist within the BERT LLM, particularly towards the African American population."
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/leeconrad_142837_9500189_CPSC_477_Final_Project_Lee_Otieno.pdf?csf=1&web=1&e=wVKyET" 

- title:  "Advancing Author-Specific Language Generation with a Custom Generative Pre-trained Transformer"
  members: "Emilia Liu, Jingjia Meng"
  abstract: "This project advances natural language processing by developing a custom Generative Pre-trained Transformer (GPT) model designed specifically to emulate Ernest Hemingway's distinctive writing style. Utilizing 'The First Forty-Nine Stories' as the training corpus, the model leverages a multi-head attention mechanism, inspired by the paper 'Attention Is All You Need'. The model's performance was evaluated using various metrics, including ROUGE, METEOR, and BERT scores, to assess its efficiency in style mimicry compared to traditional language models. Results indicated that while the model can generate text with lexical diversity and sentence complexity akin to Hemingway's style, challenges remain in capturing the full spectrum of his stylistic essence. Future work will focus on optimizing model architecture and training processes to enhance the fidelity of generated text to Hemingway's style. This approach not only demonstrates the capabilities of GPT models in personalized language modeling but also opens avenues for future research into author-specific language generation. Such developments hold significant promise for applications in digital humanities, authorial style emulation, and beyond. Code is available at:https://github.com/jjmeng08/CPSC_Project.git"
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/liuemilia_167819_9491967_CPSC_final_report.pdf?csf=1&web=1&e=VfpmNm" 

- title: "Instruction Tuning to Improve Multi-Document Processing Capabilities of LLMs"
  members: "Gabrielle Kaili-May Liu, Richard Luo"
  abstract: "Multi-document pre-training objectives are a strong approach to boosting LLM performance on downstream multi-document downstream tasks. Yet such approaches tend to be less general and scalable to broader model types and sizes. Additionally complicating multi-document task capabilities is the "lost-in-the-middle" phenomenon, whereby the performance of long-context language models decreases significantly when relevant information is located in the middle of the context as opposed to the beginning or end. Recent work suggests instruction tuning as a scalable method for enabling automatic instruction generation/following in LLMs. In this project we therefore leverage such an approach to confer LLMs with improved long-context multi-document capabilities in a more scalable way. Preliminary results demonstrate promise for our proposed approach in the 0-shot setting. Our code is available at https://github.com/pybeebee/577_final_project."
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/liukaili_195424_9498068_577_Final_Report.pdf?csf=1&web=1&e=PDfWcj"

- title: "Multimodal ClinicalEDBERT: Predicting Hospital Admissions with MIMIC-IV ED Database"
  members: "Yufei Deng, Yihan Liu, Hang Shi"
  abstract:
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/liuyihan_169918_9492645_CPSC_577_Final_project_report.pdf?csf=1&web=1&e=kBn3p8"

- title: "Advancements in NLP for Autonomous Robotic Systems through Transformer Models and Neural Networks"
  members: "Liam Merz Hoffmeister, Stephen Miner"
  abstract:
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/minerstephen_195487_9500534_Final_Project_Report.pdf?csf=1&web=1&e=RPoUf9"

- title: "Mixture-of-Experts Transformers: A Survey"
  members: "Yizheng (Jerry) Shi, Ginny Xiao, Abhisar Mittal, Ardavan (Harry) Abiri"
  abstract:
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/mittalabhisar_147890_9498554_Final%20Project.pdf?csf=1&web=1&e=rnQhtB"

- title: "Multimodal Training of Transformers"
  members: "Leo deJong, Reese Johnson, Siva Nalabothu"
  abstract: The transformer architecture by Vaswani et al. (2017) has replaced RNN-LSTM based approaches in state-of-the-art language modeling. In particular, decoder-only generative large language models between the range of 7 billion and 300 billion parameters (or more than 1 trillion for some mixture of experts models) have shown remarkable performance in mimicking human conversational abilities on a wide range of topics. One of the important directions for continuous improvement of these transformer models is natively adding support for multimodal input and output. To this end, this paper reviews state-of-the-art approaches in multimodality today and presents results related to replicating Lewkowycz et al.'s (2022) attempt to train language models to solve quantitative problems. Our code is available at https://github.com/snalabothu/multimodal-training-of-transformers.
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/nalabothusiva_146996_9490849_final_proj.pdf?csf=1&web=1&e=35plWt"

- title: "Contextual Embeddings for Sentiment Classification Accuracy"
  members: "Carl Viyar, Christopher Nathan"
  abstract: "Our project seeks to explore the benefits of contextual word embeddings for the task of sentiment classification. We base our approach on the findings of the 2017 paper 'Learned in Translation: Contextualized Word Vectors" by McCann et. al. that uses ELMo embeddings to achieve a 6.8 % increase in sentiment classification accuracy. In this paper, we compare performance using BERT-derived token embeddings with baseline performance using GloVe embeddings, finding that BERT-derived embeddings demonstrated a similar increase in improvement."
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/nathanchristopher_103084_9489343_ViyarNathan_CPSC477_FinalProject.pdf?csf=1&web=1&e=XBAF6x"
