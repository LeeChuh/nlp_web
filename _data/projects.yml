- title: "Training and Benchmarking Neural Machine Translation Models"
  members: "Ethan Mathieu, Shankara Abbineni"
  abstract: "In this project, we ask two questions: what are the gains to fine-tuning general langauge models on translation; and can general language models, when fine-tuned, perform better on translation tasks than a model trained solely for translation. As such, we train the DeLighT transformer model for English-to-French translation and compare its BLEU performance to other neural machine translation models which we fine-tune. We find that fine-tuned general language models can perform better than language-specific models. Additionally, we build a NextJS web application to allow end users to experiment with the different models and view their performance."
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/abbinenishankara_116308_9500712_CPSC477_Final_Report.pdf?csf=1&web=1&e=ahF1g0"

- title: "Improved Protein Function Prediction by Combining Persistent Cohomology and ProteinBERT Embeddings"
  members: "Anna Su, Jason Apostol"
  abstract: "Understanding the molecular function of proteins is extremely important in elucidating their biological mechanisms and in engineering new theraputics. We present a protein function classifier combining features from both sequence and structure, through embeddings generated by a pretrained ProteinBERT model trained on $\sim 100 \mathrm{M}$ proteins supplemented with structural generated on a molecular functionspecific implementation of PersLay trained on our smaller target dataset of $\sim 6,000$ human protein structures. We show that supplementing the sequence embeddings with structural embeddings improves classifier accuracy by approximately $4 \%$ by using a relatively small number of parameters, and demonstrate that the $H_1$ homology group is the most important for performance. This work has applications to drug discovery, elucidation of biological pathways, and protein engineering, as it provides a high-fidelity estimate of the role of a protein in a biological system."
  link: "https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/apostoljason_148729_9500993_NLP_Final_Project%20(2).pdf?csf=1&web=1&e=cVnHaD"

- title: "Biomedical Lay Summarization"
  members: "Xincheng Cai, Mengmeng Du"
  abstract: "Biomedical research articles contain vital information for a wide audience, yet their complex language and specialized terminology often hinder comprehension for non-experts. Inspired by the BIONLP 2024 workshop, we propose a NLP solution to generate lay summaries, which are more readable to diverse audiences. We implemented two transformer-based models, specifically BART and BART-PubMed. Our study investigates the performance of these models across different biomedical topics and explores methods to improve summarization quality through definition retrieval from Webster Medical Dictionary. By enhancing the readability of biomedical publications, our work aims to promote knowledge accessibility to scientific information."
  link: https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/caixincheng_192392_9496589_CPSC577_Final_Project.pdf?csf=1&web=1&e=6qHdOP

- title: "Advancing AI Safety in LLMs through Dynamic Multi-Agent Debates"
  members: "Vincent Li, Anna Zhang, Lindsay Chen"
  abstract: "The safety and security of large language models (LLMs) has garnered significant attention with the advent of multi-agent frameworks. Our research expands on methodologies proposed in "Combating Adversarial Attacks with Multi-Agent Debate"(1) by introducing dynamic role allocation and diversifying agent capabilities within multi-agent frameworks. These enhancements address key limitations, including static role allocation and agent homogeneity, which limit the adaptability of debates in uncovering adversarial strategies. Our proposed framework incorporates dynamic roles such as proposer, opposer, questioner, and mediator, alongside enhanced agent capabilities that allow for nuanced exploration of adversarial dialogues. The framework is implemented and trained using state-of-the-art LLMs and evaluated on existing datasets, demonstrating its effectiveness in identifying and mitigating adversarial threats in LLMs. This innovative approach advances AI safety by fostering more robust and versatile multi-agent interactions, contributing to secure and reliable LLM applications."
  link: https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/chenlindsay_108121_9496997_CPSC477_Final_Report.pdf?csf=1&web=1&e=Jt3YIA

- title: "Transfer Learning is All You Need for Sentiment Analysis"
  members: "Minyi Chen, Zishun Zhou, Bowen Duanmu"
  abstract: "Transfer learning is a crucial technique that helps us learn from external sources, thus improving model performance on small datasets. In this paper, we work on Twitter Sentiment Datasets with three categories: Neutral, Positive, and Negative, using models like Bert and Gemma, and explore the impact of transfer learning on classification performance. We experimented with various data preprocessing strategies, such as removing stop words and special characters like emojis. We pre-trained our model on different datasets with similar or different tasks. During fine-tuning, we tried various freeze strategies as well. Our best results get $93.5 \%$ accuracy, $93.1 \%$ recall, and $93.4 \%$ F1 score in test set. Experimental results indicate that the performance of transfer learning is influenced by various factors, including the model, dataset relationships, and freeze strategies."
  link: https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/chenminyi_194162_9501120_NeurIPS_2023__Copy_%20(1).pdf?csf=1&web=1&e=KSrOgh 

- title: "Deciphering Clinical Trial Reports: A Novel NLP Task and Corpus for Evidence Inference"
  members: "Xinyi Di, Chengxi Wang, Yun Yang"
  abstract: "In healthcare, accurate assessment of treatment efficacy is crucial but hindered by the complex and voluminous nature of clinical trial reports. Traditional methods fall short, highlighting the need for advanced automated solutions. Our research addresses this challenge by developing NLP models that utilize sophisticated attention mechanisms to improve the extraction and synthesis of evidence from these reports. By incorporating LoRA, we enhance the fine-tuning efficiency of our models, making large language models more accessible and effective. We evaluate our approach by comparing the performance of a BERT-based baseline model with advanced models constructed using BioBERT and ClinicalBert. This study not only advances the field of NLP in healthcare but also has the potential to revolutionize the way clinical evidence is processed, hence enhancing patient care."
  link: https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/dixinyi_168124_9488283_NLP_Final_Report.pdf?csf=1&web=1&e=WEPIz7

- title: "Llama3-8-Bing A sarcastic language model learns from Chandler Bing"
  members: "Yuntian Liu, Zihan Dong"
  abstract: "In this project, we explored various large language models and fine tuning or alignment techniques to classify and generate sarcasm dialogues. We adopted generative AI to boost the sarcasm study and trained a sarcastic chatbot based on llama3-8B model that learned from Chandler Bing."
  link: https://yaleedu-my.sharepoint.com/:b:/r/personal/arman_cohan_yale_edu/Documents/courses/cpsc477-sp24/project-reports/dongzihan_106584_9500895_CPSC577_SP24_Final_project.pdf?csf=1&web=1&e=cZFG6n

- title: 
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:


- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:


- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

- title:
  members:
  abstract:
  link:

